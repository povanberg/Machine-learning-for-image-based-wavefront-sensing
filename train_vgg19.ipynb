{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from dataset2 import PSFDataset, ToTensor, MinMaxNorm\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others\n",
    "random_seed = 42\n",
    "\n",
    "# Data variables\n",
    "data_dir = 'data/'\n",
    "dataset_size = 10000\n",
    "shuffle_dataset = True\n",
    "split = [0.9, 0.1]      # [Train, Val]\n",
    "\n",
    "# Train variables\n",
    "model_dir = 'models/baseline'\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logs\n",
    "log_path = os.path.join(model_dir, 'logs.log')\n",
    "utils.set_logger(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train set size: 9000 | Validation set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load and split dataset in training and validation sets.\n",
    "\n",
    "dataset = PSFDataset(root_dir=data_dir, size=dataset_size,\n",
    "                     transform=transforms.Compose([MinMaxNorm(), ToTensor()]))\n",
    "    \n",
    "indices = list(range(dataset_size))\n",
    "s = int(np.floor(split[1] * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[s:], indices[:s]\n",
    "    \n",
    "train_sampler, val_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=val_sampler)\n",
    "\n",
    "logging.info('Train set size: %i | Validation set size: %i' % (len(train_indices), \n",
    "                                                               len(val_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model deployed on 2 GPUs\n"
     ]
    }
   ],
   "source": [
    "# Transfer learning applied to vgg19 (128x128x3 input (3 and 1 images are the same))\n",
    "\n",
    "model = models.vgg19(pretrained=True)\n",
    "\n",
    "# Freeze weights of conv layers\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "# Replace fc layers to fit input size\n",
    "model.classifier = nn.Sequential(\n",
    "                        nn.Linear(8192, 1024),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(1024, 256),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Dropout(p=0.5),\n",
    "                        nn.Linear(256, 20)\n",
    "                    )\n",
    "\n",
    "# Deploy on multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info(\"Model deployed on %d GPUs\" % (torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 20 epochs\"\"\"\n",
    "    lr = 0.01 * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "Process Process-20:\n",
      "Process Process-17:\n",
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "Traceback (most recent call last):\n",
      "Process Process-19:\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/mnt/diskss/povanberg/phase-retrieval-v2/dataset2.py\", line 27, in __getitem__\n",
      "    sample_hdu = fits.open(sample_name)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 151, in fitsopen\n",
      "    lazy_load_hdus, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 387, in fromfile\n",
      "    lazy_load_hdus=lazy_load_hdus, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 999, in _readfrom\n",
      "    read_one = hdulist._read_next_hdu()\n",
      "  File \"/mnt/diskss/povanberg/phase-retrieval-v2/dataset2.py\", line 27, in __getitem__\n",
      "    sample_hdu = fits.open(sample_name)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1074, in _read_next_hdu\n",
      "    hdu = _BaseHDU.readfrom(fileobj, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 151, in fitsopen\n",
      "    lazy_load_hdus, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 328, in readfrom\n",
      "    **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 184, in default_collate\n",
      "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 387, in fromfile\n",
      "    lazy_load_hdus=lazy_load_hdus, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 184, in <dictcomp>\n",
      "    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 431, in _readfrom_internal\n",
      "    cls = _hdu_class_from_header(cls, header)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 999, in _readfrom\n",
      "    read_one = hdulist._read_next_hdu()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 164, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1074, in _read_next_hdu\n",
      "    hdu = _BaseHDU.readfrom(fileobj, **kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 328, in readfrom\n",
      "    **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 432, in _readfrom_internal\n",
      "    sig = signature(cls.__init__)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 3070, in signature\n",
      "    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 2820, in from_callable\n",
      "    follow_wrapper_chains=follow_wrapped)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 2277, in _signature_from_callable\n",
      "    return _signature_from_function(sigcls, obj)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/inspect.py\", line 2126, in _signature_from_function\n",
      "    pos_count = func_code.co_argcount\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fbf3e028ea0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 45644) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a720dd0ee50a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Forward pass, backward pass, optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzernike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pointwise_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, reduction)\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'elementwise_mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "    logging.info('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    logging.info('-' * 10)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    log_every = len(train_dataloader) // 2\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "\n",
    "        zernike = sample_batched['zernike'].type(torch.FloatTensor)\n",
    "        image = sample_batched['image'].type(torch.FloatTensor)\n",
    "        image = image.to(device)\n",
    "        zernike = zernike.to(device)\n",
    "\n",
    "        # Forward pass, backward pass, optimize\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, zernike)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss)\n",
    "        # Print statistics\n",
    "        if (i_batch + 1) % (log_every) == 0:\n",
    "            logging.info('train loss: %.3f time: %.3f s' %\n",
    "                      (running_loss / log_every, time.time() - epoch_time))\n",
    "            running_loss = 0.0\n",
    "            epoch_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(val_dataloader):\n",
    "\n",
    "        zernike = sample_batched['zernike'].type(torch.FloatTensor)\n",
    "        image = sample_batched['image'].type(torch.FloatTensor)\n",
    "        image = image.to(device)\n",
    "        zernike = zernike.to(device)\n",
    "\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, zernike)\n",
    "        val_loss += float(loss)\n",
    "\n",
    "    # Save best val metrics in a json file in the model directory\n",
    "    accuracy = val_loss / len(val_dataloader)\n",
    "    metrics_json_path = os.path.join(model_dir, \"metrics.json\")\n",
    "    metrics = utils.Params(metrics_json_path)\n",
    "    if not metrics.hasKey(metrics_json_path, 'accuracy') or metrics.accuracy > accuracy:\n",
    "        metrics.accuracy = accuracy\n",
    "        metrics.save(metrics_json_path)\n",
    "        checkpoint_path = os.path.join(model_dir, 'checkpoint.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "    logging.info('val loss: %.3f ' % (val_loss / len(val_dataloader)))\n",
    "    \n",
    "logging.info('Training finished in %.3f s' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
