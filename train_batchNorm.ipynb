{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import utils\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from dataset import PSFDataset, ToTensor, MinMaxNorm\n",
    "import numpy as np\n",
    "from model_batchNorm import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "n_zernike = 20\n",
    "split = 0.1\n",
    "batch_size = 256 # Increase stability of convergence?\n",
    "dataset_size = 10000\n",
    "num_epochs = 300\n",
    "lr = 0.001\n",
    "\n",
    "model_dir = 'models/baseline_norm/'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "data_dir = 'psfs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU support\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logs\n",
    "log_path = os.path.join(model_dir, 'logs.log')\n",
    "utils.set_logger(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train set size: 9216 | Validation set size: 1024\n"
     ]
    }
   ],
   "source": [
    "# Load dataset:\n",
    "dataset = PSFDataset(root_dir=data_dir, size=dataset_size,\n",
    "                         transform=transforms.Compose([MinMaxNorm(), ToTensor()]))\n",
    "# Ensure reproducibility:\n",
    "random_seed = 42\n",
    "shuffle_dataset = True\n",
    "    \n",
    "# Split train-test:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "    \n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4, sampler=val_sampler)\n",
    "\n",
    "logging.info('Train set size: %i | Validation set size: %i' % (batch_size*len(train_dataloader), \n",
    "                                                              batch_size*len(val_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model deployed on 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv11_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv22_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv33): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv33_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=32768, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Load convolutional network\n",
    "model = Net()\n",
    "#state_dict = torch.load(os.path.join(model_dir, 'checkpoint.pth'))\n",
    "#new_state_dict = OrderedDict()\n",
    "#for k, v in state_dict.items():\n",
    "#    name = k[7:] # remove module.\n",
    "#    new_state_dict[name] = v\n",
    "#model.load_state_dict(new_state_dict)\n",
    "print(model)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info(\"Model deployed on %d GPUs\" % (torch.cuda.device_count()))\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr, weight_decay=1e-3)\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 20 epochs\"\"\"\n",
    "    lr = 0.001 * (0.1 ** (epoch // 50))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/299\n",
      "----------\n",
      "estimate train loss: 12494.660 time: 4.598 s\n",
      "estimate train loss: 9675.897 time: 3.301 s\n",
      "estimate train loss: 7085.382 time: 3.060 s\n",
      "val loss: 18868.649 \n",
      "Epoch 1/299\n",
      "----------\n",
      "estimate train loss: 5290.200 time: 4.564 s\n",
      "estimate train loss: 4339.108 time: 3.324 s\n",
      "estimate train loss: 3767.070 time: 3.145 s\n",
      "val loss: 8297.976 \n",
      "Epoch 2/299\n",
      "----------\n",
      "estimate train loss: 3324.518 time: 4.728 s\n",
      "estimate train loss: 2977.151 time: 3.456 s\n",
      "estimate train loss: 2808.671 time: 3.167 s\n",
      "val loss: 2758.469 \n",
      "Epoch 3/299\n",
      "----------\n",
      "estimate train loss: 2492.915 time: 4.610 s\n",
      "estimate train loss: 2364.687 time: 3.443 s\n",
      "estimate train loss: 2225.074 time: 3.076 s\n",
      "val loss: 4672.418 \n",
      "Epoch 4/299\n",
      "----------\n",
      "estimate train loss: 2074.808 time: 4.595 s\n",
      "estimate train loss: 1894.315 time: 3.378 s\n",
      "estimate train loss: 1798.366 time: 3.207 s\n",
      "val loss: 3041.575 \n",
      "Epoch 5/299\n",
      "----------\n",
      "estimate train loss: 1680.863 time: 4.571 s\n",
      "estimate train loss: 1520.133 time: 3.339 s\n",
      "estimate train loss: 1440.248 time: 3.129 s\n",
      "val loss: 2876.480 \n",
      "Epoch 6/299\n",
      "----------\n",
      "estimate train loss: 1336.694 time: 4.631 s\n",
      "estimate train loss: 1233.573 time: 3.324 s\n",
      "estimate train loss: 1121.678 time: 3.089 s\n",
      "val loss: 2514.836 \n",
      "Epoch 7/299\n",
      "----------\n",
      "estimate train loss: 1047.944 time: 4.554 s\n",
      "estimate train loss: 973.838 time: 3.317 s\n",
      "estimate train loss: 959.233 time: 3.085 s\n",
      "val loss: 5865.171 \n",
      "Epoch 8/299\n",
      "----------\n",
      "estimate train loss: 945.281 time: 4.765 s\n",
      "estimate train loss: 890.341 time: 3.308 s\n",
      "estimate train loss: 868.470 time: 3.103 s\n",
      "val loss: 1390.232 \n",
      "Epoch 9/299\n",
      "----------\n",
      "estimate train loss: 809.016 time: 4.725 s\n",
      "estimate train loss: 768.853 time: 3.404 s\n",
      "estimate train loss: 774.984 time: 3.169 s\n",
      "val loss: 2139.071 \n",
      "Epoch 10/299\n",
      "----------\n",
      "estimate train loss: 779.740 time: 4.585 s\n",
      "estimate train loss: 732.191 time: 3.364 s\n",
      "estimate train loss: 723.836 time: 3.098 s\n",
      "val loss: 5944.035 \n",
      "Epoch 11/299\n",
      "----------\n",
      "estimate train loss: 713.613 time: 4.664 s\n",
      "estimate train loss: 666.929 time: 3.377 s\n",
      "estimate train loss: 651.348 time: 3.145 s\n",
      "val loss: 13421.553 \n",
      "Epoch 12/299\n",
      "----------\n",
      "estimate train loss: 645.543 time: 4.635 s\n",
      "estimate train loss: 607.312 time: 3.356 s\n",
      "estimate train loss: 588.106 time: 3.127 s\n",
      "val loss: 1539.981 \n",
      "Epoch 13/299\n",
      "----------\n",
      "estimate train loss: 598.262 time: 4.637 s\n",
      "estimate train loss: 582.561 time: 3.451 s\n",
      "estimate train loss: 579.419 time: 3.192 s\n",
      "val loss: 1608.872 \n",
      "Epoch 14/299\n",
      "----------\n",
      "estimate train loss: 575.601 time: 4.640 s\n",
      "estimate train loss: 561.640 time: 3.427 s\n",
      "estimate train loss: 569.944 time: 3.193 s\n",
      "val loss: 2322.364 \n",
      "Epoch 15/299\n",
      "----------\n",
      "estimate train loss: 531.616 time: 4.507 s\n",
      "estimate train loss: 528.526 time: 3.357 s\n",
      "estimate train loss: 502.963 time: 3.085 s\n",
      "val loss: 2135.982 \n",
      "Epoch 16/299\n",
      "----------\n",
      "estimate train loss: 490.212 time: 4.775 s\n",
      "estimate train loss: 487.527 time: 3.448 s\n",
      "estimate train loss: 468.037 time: 3.212 s\n",
      "val loss: 8554.620 \n",
      "Epoch 17/299\n",
      "----------\n",
      "estimate train loss: 473.846 time: 4.633 s\n",
      "estimate train loss: 459.098 time: 3.440 s\n",
      "estimate train loss: 451.596 time: 3.164 s\n",
      "val loss: 12547.426 \n",
      "Epoch 18/299\n",
      "----------\n",
      "estimate train loss: 444.043 time: 4.693 s\n",
      "estimate train loss: 420.386 time: 3.524 s\n",
      "estimate train loss: 435.282 time: 3.198 s\n",
      "val loss: 3367.043 \n",
      "Epoch 19/299\n",
      "----------\n",
      "estimate train loss: 419.450 time: 4.617 s\n",
      "estimate train loss: 426.262 time: 3.367 s\n",
      "estimate train loss: 435.421 time: 3.175 s\n",
      "val loss: 2064.310 \n",
      "Epoch 20/299\n",
      "----------\n",
      "estimate train loss: 398.696 time: 4.610 s\n",
      "estimate train loss: 357.845 time: 3.402 s\n",
      "estimate train loss: 338.514 time: 3.171 s\n",
      "val loss: 532.515 \n",
      "Epoch 21/299\n",
      "----------\n",
      "estimate train loss: 332.676 time: 4.637 s\n",
      "estimate train loss: 336.518 time: 3.438 s\n",
      "estimate train loss: 330.685 time: 3.146 s\n",
      "val loss: 484.635 \n",
      "Epoch 22/299\n",
      "----------\n",
      "estimate train loss: 331.272 time: 4.801 s\n",
      "estimate train loss: 323.759 time: 3.549 s\n",
      "estimate train loss: 325.535 time: 3.173 s\n",
      "val loss: 479.712 \n",
      "Epoch 23/299\n",
      "----------\n",
      "estimate train loss: 325.304 time: 4.590 s\n",
      "estimate train loss: 319.062 time: 3.401 s\n",
      "estimate train loss: 327.532 time: 3.137 s\n",
      "val loss: 481.956 \n",
      "Epoch 24/299\n",
      "----------\n",
      "estimate train loss: 316.085 time: 4.467 s\n",
      "estimate train loss: 319.153 time: 3.351 s\n",
      "estimate train loss: 322.853 time: 3.128 s\n",
      "val loss: 484.920 \n",
      "Epoch 25/299\n",
      "----------\n",
      "estimate train loss: 319.188 time: 4.531 s\n",
      "estimate train loss: 315.811 time: 3.341 s\n",
      "estimate train loss: 310.524 time: 3.102 s\n",
      "val loss: 458.400 \n",
      "Epoch 26/299\n",
      "----------\n",
      "estimate train loss: 312.401 time: 4.632 s\n",
      "estimate train loss: 310.557 time: 3.334 s\n",
      "estimate train loss: 313.648 time: 3.073 s\n",
      "val loss: 464.773 \n",
      "Epoch 27/299\n",
      "----------\n",
      "estimate train loss: 310.198 time: 4.731 s\n",
      "estimate train loss: 314.133 time: 3.394 s\n",
      "estimate train loss: 309.585 time: 3.121 s\n",
      "val loss: 494.846 \n",
      "Epoch 28/299\n",
      "----------\n",
      "estimate train loss: 305.208 time: 4.564 s\n",
      "estimate train loss: 308.557 time: 3.398 s\n",
      "estimate train loss: 304.027 time: 3.190 s\n",
      "val loss: 479.273 \n",
      "Epoch 29/299\n",
      "----------\n",
      "estimate train loss: 307.000 time: 4.591 s\n",
      "estimate train loss: 300.105 time: 3.473 s\n",
      "estimate train loss: 302.358 time: 3.196 s\n",
      "val loss: 470.615 \n",
      "Epoch 30/299\n",
      "----------\n",
      "estimate train loss: 302.364 time: 4.705 s\n",
      "estimate train loss: 299.919 time: 3.488 s\n",
      "estimate train loss: 302.067 time: 3.182 s\n",
      "val loss: 454.360 \n",
      "Epoch 31/299\n",
      "----------\n",
      "estimate train loss: 297.901 time: 4.619 s\n",
      "estimate train loss: 300.023 time: 3.382 s\n",
      "estimate train loss: 305.579 time: 3.109 s\n",
      "val loss: 463.969 \n",
      "Epoch 32/299\n",
      "----------\n",
      "estimate train loss: 293.139 time: 4.584 s\n",
      "estimate train loss: 298.712 time: 3.389 s\n",
      "estimate train loss: 292.280 time: 3.077 s\n",
      "val loss: 457.855 \n",
      "Epoch 33/299\n",
      "----------\n",
      "estimate train loss: 290.567 time: 4.561 s\n",
      "estimate train loss: 294.524 time: 3.364 s\n",
      "estimate train loss: 293.705 time: 3.110 s\n",
      "val loss: 508.845 \n",
      "Epoch 34/299\n",
      "----------\n",
      "estimate train loss: 285.745 time: 4.598 s\n",
      "estimate train loss: 295.264 time: 3.458 s\n",
      "estimate train loss: 285.544 time: 3.260 s\n",
      "val loss: 457.260 \n",
      "Epoch 35/299\n",
      "----------\n",
      "estimate train loss: 291.183 time: 4.606 s\n",
      "estimate train loss: 291.271 time: 3.415 s\n",
      "estimate train loss: 282.063 time: 3.150 s\n",
      "val loss: 431.777 \n",
      "Epoch 36/299\n",
      "----------\n",
      "estimate train loss: 284.100 time: 4.619 s\n",
      "estimate train loss: 278.285 time: 3.478 s\n",
      "estimate train loss: 294.242 time: 3.316 s\n",
      "val loss: 811.652 \n",
      "Epoch 37/299\n",
      "----------\n",
      "estimate train loss: 288.116 time: 4.569 s\n",
      "estimate train loss: 280.014 time: 3.452 s\n",
      "estimate train loss: 280.181 time: 3.181 s\n",
      "val loss: 476.348 \n",
      "Epoch 38/299\n",
      "----------\n",
      "estimate train loss: 279.682 time: 4.583 s\n",
      "estimate train loss: 280.279 time: 3.404 s\n",
      "estimate train loss: 284.822 time: 3.112 s\n",
      "val loss: 546.289 \n",
      "Epoch 39/299\n",
      "----------\n",
      "estimate train loss: 288.761 time: 4.681 s\n",
      "estimate train loss: 281.492 time: 3.347 s\n",
      "estimate train loss: 279.053 time: 3.083 s\n",
      "val loss: 456.552 \n",
      "Epoch 40/299\n",
      "----------\n",
      "estimate train loss: 274.906 time: 4.576 s\n",
      "estimate train loss: 272.763 time: 3.363 s\n",
      "estimate train loss: 266.478 time: 3.159 s\n",
      "val loss: 420.190 \n",
      "Epoch 41/299\n",
      "----------\n",
      "estimate train loss: 264.291 time: 4.642 s\n",
      "estimate train loss: 266.540 time: 3.340 s\n",
      "estimate train loss: 275.210 time: 3.144 s\n",
      "val loss: 419.646 \n",
      "Epoch 42/299\n",
      "----------\n",
      "estimate train loss: 269.597 time: 4.656 s\n",
      "estimate train loss: 267.653 time: 3.404 s\n",
      "estimate train loss: 262.389 time: 3.172 s\n",
      "val loss: 418.199 \n",
      "Epoch 43/299\n",
      "----------\n",
      "estimate train loss: 261.071 time: 4.670 s\n",
      "estimate train loss: 269.125 time: 3.345 s\n",
      "estimate train loss: 270.574 time: 3.119 s\n",
      "val loss: 417.742 \n",
      "Epoch 44/299\n",
      "----------\n",
      "estimate train loss: 264.844 time: 4.421 s\n",
      "estimate train loss: 265.989 time: 3.334 s\n",
      "estimate train loss: 264.343 time: 3.115 s\n",
      "val loss: 417.585 \n",
      "Epoch 45/299\n",
      "----------\n",
      "estimate train loss: 266.958 time: 4.633 s\n",
      "estimate train loss: 262.600 time: 3.455 s\n",
      "estimate train loss: 264.629 time: 3.138 s\n",
      "val loss: 419.103 \n",
      "Epoch 46/299\n",
      "----------\n",
      "estimate train loss: 266.973 time: 4.621 s\n",
      "estimate train loss: 265.922 time: 3.386 s\n",
      "estimate train loss: 269.463 time: 3.080 s\n",
      "val loss: 418.413 \n",
      "Epoch 47/299\n",
      "----------\n",
      "estimate train loss: 265.549 time: 4.750 s\n",
      "estimate train loss: 261.061 time: 3.440 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate train loss: 268.924 time: 3.127 s\n",
      "val loss: 417.080 \n",
      "Epoch 48/299\n",
      "----------\n",
      "estimate train loss: 262.004 time: 4.705 s\n",
      "estimate train loss: 265.442 time: 3.327 s\n",
      "estimate train loss: 270.016 time: 3.106 s\n",
      "val loss: 418.611 \n",
      "Epoch 49/299\n",
      "----------\n",
      "estimate train loss: 264.932 time: 4.576 s\n",
      "estimate train loss: 261.213 time: 3.369 s\n",
      "estimate train loss: 269.139 time: 3.169 s\n",
      "val loss: 420.758 \n",
      "Epoch 50/299\n",
      "----------\n",
      "estimate train loss: 262.904 time: 4.714 s\n",
      "estimate train loss: 265.233 time: 3.311 s\n",
      "estimate train loss: 260.120 time: 3.113 s\n",
      "val loss: 419.114 \n",
      "Epoch 51/299\n",
      "----------\n",
      "estimate train loss: 267.430 time: 4.713 s\n",
      "estimate train loss: 260.523 time: 3.378 s\n",
      "estimate train loss: 271.432 time: 3.158 s\n",
      "val loss: 417.781 \n",
      "Epoch 52/299\n",
      "----------\n",
      "estimate train loss: 264.136 time: 4.478 s\n",
      "estimate train loss: 266.977 time: 3.390 s\n",
      "estimate train loss: 259.135 time: 3.138 s\n",
      "val loss: 418.724 \n",
      "Epoch 53/299\n",
      "----------\n",
      "estimate train loss: 258.483 time: 4.526 s\n",
      "estimate train loss: 266.459 time: 3.531 s\n",
      "estimate train loss: 263.888 time: 3.197 s\n",
      "val loss: 415.550 \n",
      "Epoch 54/299\n",
      "----------\n",
      "estimate train loss: 260.098 time: 4.665 s\n",
      "estimate train loss: 264.119 time: 3.502 s\n",
      "estimate train loss: 264.155 time: 3.191 s\n",
      "val loss: 415.079 \n",
      "Epoch 55/299\n",
      "----------\n",
      "estimate train loss: 266.859 time: 4.676 s\n",
      "estimate train loss: 260.069 time: 3.427 s\n",
      "estimate train loss: 261.444 time: 3.196 s\n",
      "val loss: 418.984 \n",
      "Epoch 56/299\n",
      "----------\n",
      "estimate train loss: 265.419 time: 4.611 s\n",
      "estimate train loss: 260.312 time: 3.309 s\n",
      "estimate train loss: 260.873 time: 3.089 s\n",
      "val loss: 416.984 \n",
      "Epoch 57/299\n",
      "----------\n",
      "estimate train loss: 262.072 time: 4.581 s\n",
      "estimate train loss: 260.856 time: 3.374 s\n",
      "estimate train loss: 266.820 time: 3.129 s\n",
      "val loss: 415.372 \n",
      "Epoch 58/299\n",
      "----------\n",
      "estimate train loss: 261.926 time: 4.609 s\n",
      "estimate train loss: 263.033 time: 3.361 s\n",
      "estimate train loss: 265.885 time: 3.127 s\n",
      "val loss: 415.518 \n",
      "Epoch 59/299\n",
      "----------\n",
      "estimate train loss: 261.395 time: 4.550 s\n",
      "estimate train loss: 259.056 time: 3.419 s\n",
      "estimate train loss: 262.610 time: 3.123 s\n",
      "val loss: 414.987 \n",
      "Epoch 60/299\n",
      "----------\n",
      "estimate train loss: 259.484 time: 4.602 s\n",
      "estimate train loss: 263.876 time: 3.423 s\n",
      "estimate train loss: 255.770 time: 3.168 s\n",
      "val loss: 414.650 \n",
      "Epoch 61/299\n",
      "----------\n",
      "estimate train loss: 262.473 time: 4.580 s\n",
      "estimate train loss: 258.219 time: 3.296 s\n",
      "estimate train loss: 256.104 time: 3.060 s\n",
      "val loss: 413.241 \n",
      "Epoch 62/299\n",
      "----------\n",
      "estimate train loss: 256.694 time: 4.626 s\n",
      "estimate train loss: 260.611 time: 3.380 s\n",
      "estimate train loss: 264.905 time: 3.154 s\n",
      "val loss: 413.337 \n",
      "Epoch 63/299\n",
      "----------\n",
      "estimate train loss: 259.196 time: 4.667 s\n",
      "estimate train loss: 260.517 time: 3.432 s\n",
      "estimate train loss: 257.898 time: 3.125 s\n",
      "val loss: 413.148 \n",
      "Epoch 64/299\n",
      "----------\n",
      "estimate train loss: 258.805 time: 4.555 s\n",
      "estimate train loss: 259.480 time: 3.445 s\n",
      "estimate train loss: 263.364 time: 3.269 s\n",
      "val loss: 413.691 \n",
      "Epoch 65/299\n",
      "----------\n",
      "estimate train loss: 260.596 time: 4.652 s\n",
      "estimate train loss: 258.412 time: 3.404 s\n",
      "estimate train loss: 263.339 time: 3.131 s\n",
      "val loss: 413.115 \n",
      "Epoch 66/299\n",
      "----------\n",
      "estimate train loss: 259.004 time: 4.565 s\n",
      "estimate train loss: 261.236 time: 3.273 s\n",
      "estimate train loss: 260.577 time: 3.131 s\n",
      "val loss: 415.153 \n",
      "Epoch 67/299\n",
      "----------\n",
      "estimate train loss: 257.518 time: 4.503 s\n",
      "estimate train loss: 259.079 time: 3.278 s\n",
      "estimate train loss: 259.800 time: 3.071 s\n",
      "val loss: 413.884 \n",
      "Epoch 68/299\n",
      "----------\n",
      "estimate train loss: 260.601 time: 4.700 s\n",
      "estimate train loss: 260.634 time: 3.395 s\n",
      "estimate train loss: 258.275 time: 3.153 s\n",
      "val loss: 413.426 \n",
      "Epoch 69/299\n",
      "----------\n",
      "estimate train loss: 260.278 time: 4.552 s\n",
      "estimate train loss: 262.456 time: 3.375 s\n",
      "estimate train loss: 256.832 time: 3.100 s\n",
      "val loss: 413.732 \n",
      "Epoch 70/299\n",
      "----------\n",
      "estimate train loss: 260.623 time: 4.564 s\n",
      "estimate train loss: 259.181 time: 3.394 s\n",
      "estimate train loss: 258.058 time: 3.181 s\n",
      "val loss: 413.813 \n",
      "Epoch 71/299\n",
      "----------\n",
      "estimate train loss: 258.714 time: 4.697 s\n",
      "estimate train loss: 257.801 time: 3.484 s\n",
      "estimate train loss: 263.876 time: 3.168 s\n",
      "val loss: 413.385 \n",
      "Epoch 72/299\n",
      "----------\n",
      "estimate train loss: 262.935 time: 4.581 s\n",
      "estimate train loss: 254.179 time: 3.376 s\n",
      "estimate train loss: 266.327 time: 3.089 s\n",
      "val loss: 413.355 \n",
      "Epoch 73/299\n",
      "----------\n",
      "estimate train loss: 259.232 time: 4.694 s\n",
      "estimate train loss: 260.484 time: 3.474 s\n",
      "estimate train loss: 259.132 time: 3.143 s\n",
      "val loss: 414.122 \n",
      "Epoch 74/299\n",
      "----------\n",
      "estimate train loss: 255.733 time: 4.591 s\n",
      "estimate train loss: 259.002 time: 3.335 s\n",
      "estimate train loss: 261.926 time: 3.100 s\n",
      "val loss: 414.316 \n",
      "Epoch 75/299\n",
      "----------\n",
      "estimate train loss: 260.652 time: 4.672 s\n",
      "estimate train loss: 255.564 time: 3.322 s\n",
      "estimate train loss: 259.558 time: 3.093 s\n",
      "val loss: 413.572 \n",
      "Epoch 76/299\n",
      "----------\n",
      "estimate train loss: 256.718 time: 4.703 s\n",
      "estimate train loss: 258.252 time: 3.428 s\n",
      "estimate train loss: 263.416 time: 3.144 s\n",
      "val loss: 413.147 \n",
      "Epoch 77/299\n",
      "----------\n",
      "estimate train loss: 256.894 time: 4.721 s\n",
      "estimate train loss: 259.222 time: 3.428 s\n",
      "estimate train loss: 260.703 time: 3.159 s\n",
      "val loss: 413.176 \n",
      "Epoch 78/299\n",
      "----------\n",
      "estimate train loss: 259.142 time: 4.644 s\n",
      "estimate train loss: 261.104 time: 3.282 s\n",
      "estimate train loss: 257.464 time: 3.076 s\n",
      "val loss: 412.956 \n",
      "Epoch 79/299\n",
      "----------\n",
      "estimate train loss: 262.519 time: 4.632 s\n",
      "estimate train loss: 255.955 time: 3.370 s\n",
      "estimate train loss: 258.185 time: 3.123 s\n",
      "val loss: 413.608 \n",
      "Epoch 80/299\n",
      "----------\n",
      "estimate train loss: 257.965 time: 4.509 s\n",
      "estimate train loss: 261.811 time: 3.289 s\n",
      "estimate train loss: 260.952 time: 3.110 s\n",
      "val loss: 413.160 \n",
      "Epoch 81/299\n",
      "----------\n",
      "estimate train loss: 259.533 time: 4.644 s\n",
      "estimate train loss: 257.503 time: 3.349 s\n",
      "estimate train loss: 261.679 time: 3.098 s\n",
      "val loss: 413.313 \n",
      "Epoch 82/299\n",
      "----------\n",
      "estimate train loss: 262.016 time: 4.605 s\n",
      "estimate train loss: 258.282 time: 3.449 s\n",
      "estimate train loss: 259.052 time: 3.136 s\n",
      "val loss: 413.991 \n",
      "Epoch 83/299\n",
      "----------\n",
      "estimate train loss: 259.347 time: 4.729 s\n",
      "estimate train loss: 256.889 time: 3.312 s\n",
      "estimate train loss: 260.745 time: 3.152 s\n",
      "val loss: 413.566 \n",
      "Epoch 84/299\n",
      "----------\n",
      "estimate train loss: 255.237 time: 4.830 s\n",
      "estimate train loss: 260.902 time: 3.453 s\n",
      "estimate train loss: 261.558 time: 3.107 s\n",
      "val loss: 412.506 \n",
      "Epoch 85/299\n",
      "----------\n",
      "estimate train loss: 259.821 time: 4.834 s\n",
      "estimate train loss: 261.346 time: 3.401 s\n",
      "estimate train loss: 257.998 time: 3.154 s\n",
      "val loss: 413.118 \n",
      "Epoch 86/299\n",
      "----------\n",
      "estimate train loss: 254.348 time: 4.913 s\n",
      "estimate train loss: 261.525 time: 3.349 s\n",
      "estimate train loss: 261.670 time: 3.161 s\n",
      "val loss: 414.089 \n",
      "Epoch 87/299\n",
      "----------\n",
      "estimate train loss: 258.982 time: 4.649 s\n",
      "estimate train loss: 259.808 time: 3.411 s\n",
      "estimate train loss: 256.399 time: 3.138 s\n",
      "val loss: 415.005 \n",
      "Epoch 88/299\n",
      "----------\n",
      "estimate train loss: 256.846 time: 4.673 s\n",
      "estimate train loss: 263.317 time: 3.397 s\n",
      "estimate train loss: 259.115 time: 3.176 s\n",
      "val loss: 413.144 \n",
      "Epoch 89/299\n",
      "----------\n",
      "estimate train loss: 261.063 time: 4.585 s\n",
      "estimate train loss: 254.972 time: 3.335 s\n",
      "estimate train loss: 262.419 time: 3.082 s\n",
      "val loss: 413.112 \n",
      "Epoch 90/299\n",
      "----------\n",
      "estimate train loss: 258.421 time: 4.599 s\n",
      "estimate train loss: 256.795 time: 3.427 s\n",
      "estimate train loss: 261.188 time: 3.193 s\n",
      "val loss: 413.721 \n",
      "Epoch 91/299\n",
      "----------\n",
      "estimate train loss: 260.680 time: 4.648 s\n",
      "estimate train loss: 259.848 time: 3.351 s\n",
      "estimate train loss: 256.889 time: 3.104 s\n",
      "val loss: 413.645 \n",
      "Epoch 92/299\n",
      "----------\n",
      "estimate train loss: 254.855 time: 4.691 s\n",
      "estimate train loss: 259.509 time: 3.253 s\n",
      "estimate train loss: 264.653 time: 3.132 s\n",
      "val loss: 413.935 \n",
      "Epoch 93/299\n",
      "----------\n",
      "estimate train loss: 257.241 time: 4.717 s\n",
      "estimate train loss: 258.074 time: 3.386 s\n",
      "estimate train loss: 259.366 time: 3.182 s\n",
      "val loss: 413.855 \n",
      "Epoch 94/299\n",
      "----------\n",
      "estimate train loss: 257.774 time: 4.786 s\n",
      "estimate train loss: 261.810 time: 3.272 s\n",
      "estimate train loss: 262.526 time: 3.107 s\n",
      "val loss: 413.278 \n",
      "Epoch 95/299\n",
      "----------\n",
      "estimate train loss: 258.753 time: 4.617 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate train loss: 260.605 time: 3.350 s\n",
      "estimate train loss: 260.145 time: 3.112 s\n",
      "val loss: 413.850 \n",
      "Epoch 96/299\n",
      "----------\n",
      "estimate train loss: 259.120 time: 4.594 s\n",
      "estimate train loss: 259.225 time: 3.407 s\n",
      "estimate train loss: 258.577 time: 3.156 s\n",
      "val loss: 413.418 \n",
      "Epoch 97/299\n",
      "----------\n",
      "estimate train loss: 256.891 time: 4.690 s\n",
      "estimate train loss: 260.926 time: 3.514 s\n",
      "estimate train loss: 257.341 time: 3.216 s\n",
      "val loss: 412.825 \n",
      "Epoch 98/299\n",
      "----------\n",
      "estimate train loss: 260.258 time: 4.609 s\n",
      "estimate train loss: 257.646 time: 3.288 s\n",
      "estimate train loss: 257.505 time: 3.127 s\n",
      "val loss: 413.112 \n",
      "Epoch 99/299\n",
      "----------\n",
      "estimate train loss: 253.604 time: 4.659 s\n",
      "estimate train loss: 262.094 time: 3.343 s\n",
      "estimate train loss: 266.250 time: 3.064 s\n",
      "val loss: 415.647 \n",
      "Epoch 100/299\n",
      "----------\n",
      "estimate train loss: 255.050 time: 4.665 s\n",
      "estimate train loss: 261.768 time: 3.408 s\n",
      "estimate train loss: 263.462 time: 3.105 s\n",
      "val loss: 413.946 \n",
      "Epoch 101/299\n",
      "----------\n",
      "estimate train loss: 259.338 time: 4.468 s\n",
      "estimate train loss: 260.149 time: 3.467 s\n",
      "estimate train loss: 258.805 time: 3.111 s\n",
      "val loss: 412.895 \n",
      "Epoch 102/299\n",
      "----------\n",
      "estimate train loss: 263.112 time: 4.652 s\n",
      "estimate train loss: 258.151 time: 3.266 s\n",
      "estimate train loss: 256.852 time: 3.140 s\n",
      "val loss: 413.556 \n",
      "Epoch 103/299\n",
      "----------\n",
      "estimate train loss: 262.177 time: 4.738 s\n",
      "estimate train loss: 258.691 time: 3.278 s\n",
      "estimate train loss: 264.010 time: 3.125 s\n",
      "val loss: 413.567 \n",
      "Epoch 104/299\n",
      "----------\n",
      "estimate train loss: 257.236 time: 4.572 s\n",
      "estimate train loss: 258.889 time: 3.444 s\n",
      "estimate train loss: 259.091 time: 3.187 s\n",
      "val loss: 413.138 \n",
      "Epoch 105/299\n",
      "----------\n",
      "estimate train loss: 261.711 time: 4.592 s\n",
      "estimate train loss: 260.098 time: 3.306 s\n",
      "estimate train loss: 256.397 time: 3.079 s\n",
      "val loss: 413.852 \n",
      "Epoch 106/299\n",
      "----------\n",
      "estimate train loss: 261.532 time: 4.595 s\n",
      "estimate train loss: 258.792 time: 3.483 s\n",
      "estimate train loss: 258.334 time: 3.150 s\n",
      "val loss: 412.491 \n",
      "Epoch 107/299\n",
      "----------\n",
      "estimate train loss: 256.301 time: 4.692 s\n",
      "estimate train loss: 260.475 time: 3.380 s\n",
      "estimate train loss: 259.580 time: 3.109 s\n",
      "val loss: 414.218 \n",
      "Epoch 108/299\n",
      "----------\n",
      "estimate train loss: 262.518 time: 4.626 s\n",
      "estimate train loss: 254.900 time: 3.408 s\n",
      "estimate train loss: 260.872 time: 3.115 s\n",
      "val loss: 412.955 \n",
      "Epoch 109/299\n",
      "----------\n",
      "estimate train loss: 256.887 time: 4.624 s\n",
      "estimate train loss: 262.814 time: 3.434 s\n",
      "estimate train loss: 255.238 time: 3.137 s\n",
      "val loss: 412.935 \n",
      "Epoch 110/299\n",
      "----------\n",
      "estimate train loss: 254.945 time: 4.668 s\n",
      "estimate train loss: 260.697 time: 3.458 s\n",
      "estimate train loss: 260.429 time: 3.186 s\n",
      "val loss: 413.372 \n",
      "Epoch 111/299\n",
      "----------\n",
      "estimate train loss: 258.952 time: 4.766 s\n",
      "estimate train loss: 261.054 time: 3.412 s\n",
      "estimate train loss: 255.181 time: 3.104 s\n",
      "val loss: 413.547 \n",
      "Epoch 112/299\n",
      "----------\n",
      "estimate train loss: 258.546 time: 4.522 s\n",
      "estimate train loss: 260.333 time: 3.331 s\n",
      "estimate train loss: 259.617 time: 3.122 s\n",
      "val loss: 413.739 \n",
      "Epoch 113/299\n",
      "----------\n",
      "estimate train loss: 262.801 time: 4.569 s\n",
      "estimate train loss: 253.726 time: 3.373 s\n",
      "estimate train loss: 263.645 time: 3.098 s\n",
      "val loss: 412.824 \n",
      "Epoch 114/299\n",
      "----------\n",
      "estimate train loss: 258.339 time: 4.681 s\n",
      "estimate train loss: 253.499 time: 3.387 s\n",
      "estimate train loss: 265.869 time: 3.126 s\n",
      "val loss: 413.323 \n",
      "Epoch 115/299\n",
      "----------\n",
      "estimate train loss: 263.306 time: 4.618 s\n",
      "estimate train loss: 258.645 time: 3.517 s\n",
      "estimate train loss: 256.278 time: 3.181 s\n",
      "val loss: 413.965 \n",
      "Epoch 116/299\n",
      "----------\n",
      "estimate train loss: 254.623 time: 4.614 s\n",
      "estimate train loss: 261.521 time: 3.330 s\n",
      "estimate train loss: 260.290 time: 3.165 s\n",
      "val loss: 413.724 \n",
      "Epoch 117/299\n",
      "----------\n",
      "estimate train loss: 257.657 time: 4.717 s\n",
      "estimate train loss: 261.125 time: 3.460 s\n",
      "estimate train loss: 258.682 time: 3.211 s\n",
      "val loss: 412.771 \n",
      "Epoch 118/299\n",
      "----------\n",
      "estimate train loss: 259.704 time: 4.563 s\n",
      "Process Process-1933:\n",
      "Process Process-1936:\n",
      "Process Process-1935:\n",
      "Process Process-1934:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1031, in _try_while_unread_hdus\n",
      "    return func(*args, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1031, in _try_while_unread_hdus\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "IndexError: list index out of range\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "IndexError: list index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/mnt/diskss/povanberg/phase-retrieval/dataset.py\", line 29, in __getitem__\n",
      "    image = np.stack((sample_hdu[1].data, sample_hdu[2].data))\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "  File \"/mnt/diskss/povanberg/phase-retrieval/dataset.py\", line 29, in __getitem__\n",
      "    image = np.stack((sample_hdu[1].data, sample_hdu[2].data))\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 300, in __getitem__\n",
      "    self._positive_index_of(key))\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 300, in __getitem__\n",
      "    self._positive_index_of(key))\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1033, in _try_while_unread_hdus\n",
      "    if self._read_next_hdu():\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1033, in _try_while_unread_hdus\n",
      "    if self._read_next_hdu():\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1074, in _read_next_hdu\n",
      "    hdu = _BaseHDU.readfrom(fileobj, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/hdulist.py\", line 1074, in _read_next_hdu\n",
      "    hdu = _BaseHDU.readfrom(fileobj, **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 328, in readfrom\n",
      "    **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 328, in readfrom\n",
      "    **kwargs)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 431, in _readfrom_internal\n",
      "    cls = _hdu_class_from_header(cls, header)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 393, in _readfrom_internal\n",
      "    header = Header.fromfile(data, endcard=not ignore_missing_end)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/header.py\", line 447, in fromfile\n",
      "    padding)[1]\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/base.py\", line 71, in _hdu_class_from_header\n",
      "    if c.match_header(header):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/header.py\", line 493, in _from_blocks\n",
      "    end_found, block = cls._find_end_card(block, clen)\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/hdu/table.py\", line 741, in match_header\n",
      "    xtension = card.value\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/header.py\", line 560, in _find_end_card\n",
      "    for mo in HEADER_END_RE.finditer(block):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/card.py\", line 284, in value\n",
      "    self._value = self._parse_value()\n",
      "KeyboardInterrupt\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/card.py\", line 723, in _parse_value\n",
      "    if self._check_if_rvkc(self._image):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/io/fits/card.py\", line 603, in _check_if_rvkc\n",
      "    if not conf.enable_record_valued_keyword_cards:\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/config/configuration.py\", line 273, in __get__\n",
      "    return self()\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/astropy/config/configuration.py\", line 396, in __call__\n",
      "    sec = get_config(self.module)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fb95f7117b8>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/mnt/diskss/povanberg/miniconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 21948) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-24209421da3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    \n",
    "    logging.info('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    logging.info('-' * 10)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    log_every = len(train_dataloader) // 3\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "\n",
    "        zernike = sample_batched['zernike'].type(torch.FloatTensor)\n",
    "        image = sample_batched['image'].type(torch.FloatTensor)\n",
    "        image = image.to(device)\n",
    "        zernike = zernike.to(device)\n",
    "\n",
    "        # Forward pass, backward pass, optimize\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, zernike)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += float(loss)\n",
    "        # Print statistics\n",
    "        if (i_batch + 1) % (log_every) == 0:\n",
    "            logging.info('estimate train loss: %.3f time: %.3f s' %\n",
    "                      (running_loss / log_every, time.time() - epoch_time))\n",
    "            running_loss = 0.0\n",
    "            epoch_time = time.time()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(val_dataloader):\n",
    "\n",
    "        zernike = sample_batched['zernike'].type(torch.FloatTensor)\n",
    "        image = sample_batched['image'].type(torch.FloatTensor)\n",
    "        image = image.to(device)\n",
    "        zernike = zernike.to(device)\n",
    "\n",
    "        outputs = model(image)\n",
    "        loss = criterion(outputs, zernike)\n",
    "        val_loss += float(loss)\n",
    "\n",
    "    # Save best val metrics in a json file in the model directory\n",
    "    accuracy = val_loss / len(val_dataloader)\n",
    "    metrics_json_path = os.path.join(model_dir, \"metrics.json\")\n",
    "    metrics = utils.Params(metrics_json_path)\n",
    "    if not metrics.hasKey(metrics_json_path, 'accuracy') or metrics.accuracy > accuracy:\n",
    "        metrics.accuracy = accuracy\n",
    "        metrics.save(metrics_json_path)\n",
    "        checkpoint_path = os.path.join(model_dir, 'checkpoint.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        \n",
    "    logging.info('val loss: %.3f ' % (val_loss / len(val_dataloader)))\n",
    "    \n",
    "logging.info('Training finished in %.3f s' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
